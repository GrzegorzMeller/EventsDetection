{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NER_LSTM_EVALUATION.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM9woXjNIDkjgCeryDYSswi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GrzegorzMeller/EventsDetection/blob/master/NER_LSTM_EVALUATION.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lb4GMNBGI0YZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "969ad965-f44d-46e4-ffa3-2ad14115d1ba"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/amd/')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /amd/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1zaWd5QI9rn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        },
        "outputId": "1518bc80-be0c-41fa-bc51-12c09df1326b"
      },
      "source": [
        "!cp /amd/My\\ Drive/lstm_results.zip /content/\n",
        "!unzip lstm_results.zip\n",
        "\n",
        "!cp /amd/My\\ Drive/data_lstm.zip /content/\n",
        "!unzip data_lstm.zip"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  lstm_results.zip\n",
            "   creating: results/\n",
            "   creating: results/model/\n",
            "   creating: results/model/eval/\n",
            "  inflating: results/model/eval/events.out.tfevents.1594642470.bae4fb2c523c  \n",
            "  inflating: results/model/model.ckpt-604.data-00000-of-00001  \n",
            "  inflating: results/model/checkpoint  \n",
            "  inflating: results/model/model.ckpt-604.index  \n",
            "  inflating: results/model/model.ckpt-2480.data-00000-of-00001  \n",
            "  inflating: results/model/model.ckpt-2474.data-00000-of-00001  \n",
            "  inflating: results/model/model.ckpt-1223.meta  \n",
            "  inflating: results/model/model.ckpt-2480.meta  \n",
            "  inflating: results/model/events.out.tfevents.1594642344.bae4fb2c523c  \n",
            "  inflating: results/model/model.ckpt-1846.meta  \n",
            "  inflating: results/model/model.ckpt-1846.data-00000-of-00001  \n",
            "  inflating: results/model/model.ckpt-2474.index  \n",
            "  inflating: results/model/model.ckpt-1223.data-00000-of-00001  \n",
            "  inflating: results/model/model.ckpt-1846.index  \n",
            "  inflating: results/model/model.ckpt-2480.index  \n",
            "  inflating: results/model/graph.pbtxt  \n",
            "  inflating: results/model/model.ckpt-2474.meta  \n",
            "  inflating: results/model/model.ckpt-1223.index  \n",
            "  inflating: results/model/model.ckpt-604.meta  \n",
            "  inflating: results/main.log        \n",
            "  inflating: results/params.json     \n",
            "   creating: results/score/\n",
            "  inflating: results/score/dev.preds.txt  \n",
            "  inflating: results/score/test.preds.txt  \n",
            "  inflating: results/score/train.preds.txt  \n",
            "Archive:  data_lstm.zip\n",
            "  inflating: data_lstm/dev.tags.txt  \n",
            "  inflating: data_lstm/dev.words.txt  \n",
            "  inflating: data_lstm/glove.npz     \n",
            "  inflating: data_lstm/test.tags.txt  \n",
            "  inflating: data_lstm/test.words.txt  \n",
            "  inflating: data_lstm/train.tags.txt  \n",
            "  inflating: data_lstm/train.words.txt  \n",
            "  inflating: data_lstm/vocab.chars.txt  \n",
            "  inflating: data_lstm/vocab.tags.txt  \n",
            "  inflating: data_lstm/vocab.words.txt  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58aABXAGJDl-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "779f6856-5119-4540-d803-fdaeb41d1031"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n",
            "1.15.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TgksL_BENq5U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import functools\n",
        "import json\n",
        "import logging\n",
        "from pathlib import Path\n",
        "import sys\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "def model_fn(features, labels, mode, params):\n",
        "    # For serving, features are a bit different\n",
        "    if isinstance(features, dict):\n",
        "        features = features['words'], features['nwords']\n",
        "\n",
        "    # Read vocabs and inputs\n",
        "    dropout = params['dropout']\n",
        "    words, nwords = features\n",
        "    training = (mode == tf.estimator.ModeKeys.TRAIN)\n",
        "    vocab_words = tf.contrib.lookup.index_table_from_file(\n",
        "        params['words'], num_oov_buckets=params['num_oov_buckets'])\n",
        "    with Path(params['tags']).open() as f:\n",
        "        indices = [idx for idx, tag in enumerate(f) if tag.strip() != 'O']\n",
        "        num_tags = len(indices) + 1\n",
        "\n",
        "    # Word Embeddings\n",
        "    word_ids = vocab_words.lookup(words)\n",
        "    glove = np.load(params['glove'])['embeddings']  # np.array\n",
        "    variable = np.vstack([glove, [[0.]*params['dim']]])\n",
        "    variable = tf.Variable(variable, dtype=tf.float32, trainable=False)\n",
        "    embeddings = tf.nn.embedding_lookup(variable, word_ids)\n",
        "    embeddings = tf.layers.dropout(embeddings, rate=dropout, training=training)\n",
        "\n",
        "    # LSTM\n",
        "    t = tf.transpose(embeddings, perm=[1, 0, 2])\n",
        "    lstm_cell_fw = tf.contrib.rnn.LSTMBlockFusedCell(params['lstm_size'])\n",
        "    lstm_cell_bw = tf.contrib.rnn.LSTMBlockFusedCell(params['lstm_size'])\n",
        "    lstm_cell_bw = tf.contrib.rnn.TimeReversedFusedRNN(lstm_cell_bw)\n",
        "    output_fw, _ = lstm_cell_fw(t, dtype=tf.float32, sequence_length=nwords)\n",
        "    output_bw, _ = lstm_cell_bw(t, dtype=tf.float32, sequence_length=nwords)\n",
        "    output = tf.concat([output_fw, output_bw], axis=-1)\n",
        "    output = tf.transpose(output, perm=[1, 0, 2])\n",
        "    output = tf.layers.dropout(output, rate=dropout, training=training)\n",
        "\n",
        "    # CRF\n",
        "    logits = tf.layers.dense(output, num_tags)\n",
        "    crf_params = tf.get_variable(\"crf\", [num_tags, num_tags], dtype=tf.float32)\n",
        "    pred_ids, _ = tf.contrib.crf.crf_decode(logits, crf_params, nwords)\n",
        "\n",
        "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
        "        # Predictions\n",
        "        reverse_vocab_tags = tf.contrib.lookup.index_to_string_table_from_file(\n",
        "            params['tags'])\n",
        "        pred_strings = reverse_vocab_tags.lookup(tf.to_int64(pred_ids))\n",
        "        predictions = {\n",
        "            'pred_ids': pred_ids,\n",
        "            'tags': pred_strings\n",
        "        }\n",
        "        return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n",
        "    else:\n",
        "        # Loss\n",
        "        vocab_tags = tf.contrib.lookup.index_table_from_file(params['tags'])\n",
        "        tags = vocab_tags.lookup(labels)\n",
        "        log_likelihood, _ = tf.contrib.crf.crf_log_likelihood(\n",
        "            logits, tags, nwords, crf_params)\n",
        "        loss = tf.reduce_mean(-log_likelihood)\n",
        "\n",
        "        # Metrics\n",
        "        weights = tf.sequence_mask(nwords)\n",
        "        metrics = {\n",
        "            'acc': tf.metrics.accuracy(tags, pred_ids, weights),\n",
        "            'precision': precision(tags, pred_ids, num_tags, indices, weights),\n",
        "            'recall': recall(tags, pred_ids, num_tags, indices, weights),\n",
        "            'f1': f1(tags, pred_ids, num_tags, indices, weights),\n",
        "        }\n",
        "        for metric_name, op in metrics.items():\n",
        "            tf.summary.scalar(metric_name, op[1])\n",
        "\n",
        "        if mode == tf.estimator.ModeKeys.EVAL:\n",
        "            return tf.estimator.EstimatorSpec(\n",
        "                mode, loss=loss, eval_metric_ops=metrics)\n",
        "\n",
        "        elif mode == tf.estimator.ModeKeys.TRAIN:\n",
        "            train_op = tf.train.AdamOptimizer().minimize(\n",
        "                loss, global_step=tf.train.get_or_create_global_step())\n",
        "            return tf.estimator.EstimatorSpec(\n",
        "                mode, loss=loss, train_op=train_op)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSEkUEbxOF6S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LINE = 'The Siege of Marienburg was an unsuccessful two-month siege of the castle in Marienburg (Malbork), the capital of the monastic state of the Teutonic Knights. The joint Polish and Lithuanian forces, under command of King Władysław II Jagiełło and Grand Duke Vytautas, besieged the castle between 26 July and 19 September 1410 in a bid of complete conquest of Prussia after the great victory in the Battle of Grunwald (Tannenberg). However, the castle withstood the siege and the Knights conceded only to minor territorial losses in the Peace of Thorn (1411). Marienburg defender Heinrich von Plauen is credited as the savior of the Knights from complete annihilation.'"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0e4LRjIJFoC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        },
        "outputId": "625ab31d-6f20-482a-ad79-68017fc9bd5e"
      },
      "source": [
        "from pathlib import Path\n",
        "import functools\n",
        "import json\n",
        "\n",
        "DATADIR = '/content/data_lstm/'\n",
        "PARAMS = '/content/results/params.json'\n",
        "MODELDIR = '/content/results/model'\n",
        "\n",
        "\n",
        "def pretty_print(line, preds):\n",
        "    words = line.strip().split()\n",
        "    lengths = [max(len(w), len(p)) for w, p in zip(words, preds)]\n",
        "    padded_words = [w + (l - len(w)) * ' ' for w, l in zip(words, lengths)]\n",
        "    padded_preds = [p.decode() + (l - len(p)) * ' ' for p, l in zip(preds, lengths)]\n",
        "    print('words: {}'.format(' '.join(padded_words)))\n",
        "    print('preds: {}'.format(' '.join(padded_preds)))\n",
        "\n",
        "\n",
        "def predict_input_fn(line):\n",
        "    # Words\n",
        "    words = [w.encode() for w in line.strip().split()]\n",
        "    nwords = len(words)\n",
        "\n",
        "    # Wrapping in Tensors\n",
        "    words = tf.constant([words], dtype=tf.string)\n",
        "    nwords = tf.constant([nwords], dtype=tf.int32)\n",
        "\n",
        "    return (words, nwords), None\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    with Path(PARAMS).open() as f:\n",
        "        params = json.load(f)\n",
        "\n",
        "    params['words'] = str(Path(DATADIR, 'vocab.words.txt'))\n",
        "    params['chars'] = str(Path(DATADIR, 'vocab.chars.txt'))\n",
        "    params['tags'] = str(Path(DATADIR, 'vocab.tags.txt'))\n",
        "    params['glove'] = str(Path(DATADIR, 'glove.npz'))\n",
        "\n",
        "    estimator = tf.estimator.Estimator(model_fn, MODELDIR, params=params)\n",
        "    predict_inpf = functools.partial(predict_input_fn, LINE)\n",
        "    for pred in estimator.predict(predict_inpf):\n",
        "        pretty_print(LINE, pred['tags'])\n",
        "        break"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using default config.\n",
            "INFO:tensorflow:Using config: {'_model_dir': '/content/results/model', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fd2643df898>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
            "WARNING:tensorflow:Input graph does not use tf.data.Dataset or contain a QueueRunner. That means predict yields forever. This is probably a mistake.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from /content/results/model/model.ckpt-2480\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "words: The Siege of Marienburg was                  an unsuccessful two-month siege of the castle in Marienburg (Malbork), the capital of the monastic state of the Teutonic Knights. The joint Polish and Lithuanian forces, under command of King Władysław II Jagiełło and Grand Duke Vytautas, besieged the castle between 26 July and 19 September 1410 in a bid of complete conquest of Prussia after the great victory in the Battle      of Grunwald (Tannenberg). However, the castle withstood the siege and the Knights conceded only to minor    territorial losses               in the Peace of Thorn (1411). Marienburg defender Heinrich von Plauen is credited as the savior of the Knights from complete annihilation.\n",
            "preds: O   O     O  O          B-EXISTENCECAUSATION O  O            O         O     O  O   O      O  O          O          O   O       O  O   O        O     O  O   O        O        O   O     O      O   O          O       O     O       O  O    O         O  O        O   O     O    O         O        O   O      O       O  O    O   O  O         O    O  O O   O  O        O        O  O       O     O   O     O       O  O   B-HOSTILITY O  O        O             O        O   O      O         O   O     O   O   O       O        O    O  B-ACTION O           B-EXISTENCECAUSATION O  O   O     O  O     O       O          O        O        O   O      O  B-MENTAL O  O   O      O  O   O       O    O        O            \n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}